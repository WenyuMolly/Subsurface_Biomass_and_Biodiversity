{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code estimates using a predicted N_max and lognormal model of biodiversity\n",
    "## Adapted from Fig3.py from https://github.com/LennonLab/ScalingMicroBiodiversity; Locey and Lennon, 2016 (LL2016)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "import scipy as sc\n",
    "from scipy import stats\n",
    "import os\n",
    "import sys\n",
    "import statsmodels.stats.api as sms\n",
    "import statsmodels.formula.api as smf\n",
    "from statsmodels.sandbox.regression.predstd import wls_prediction_std\n",
    "from statsmodels.stats.outliers_influence import summary_table\n",
    "from numpy import log, log2, exp, sqrt, log10, pi\n",
    "from scipy.optimize import fsolve\n",
    "import scipy.optimize as opt\n",
    "import pandas as pd #import patsy\n",
    "import mpmath as mpm\n",
    "from scipy.optimize import fsolve\n",
    "from math import erf, pi\n",
    "import linecache\n",
    "import math\n",
    "from statsmodels.iolib.smpickle import load_pickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Added an intermediate function find x0 guess for opt.fsolve function\n",
    "def findGuess(N, Nmax, Nmin=1):\n",
    "    tenthRange = np.arange(0.0, 0.3, 0.01)\n",
    "    vals = []\n",
    "    for i in tenthRange:\n",
    "        vals.append(alphaFunction(i,N,Nmax,Nmin))\n",
    "    lowestErrIndex = np.argmin(abs(np.asarray(vals)))\n",
    "    bestGuess = tenthRange[lowestErrIndex]\n",
    "    hundredthRange = np.arange(bestGuess-0.05, bestGuess+0.05,0.001)\n",
    "    vals = []\n",
    "    for i in tenthRange:\n",
    "        vals.append(alphaFunction(i,N,Nmax,Nmin))\n",
    "    lowestErrIndex = np.argmin(abs(np.asarray(vals)))\n",
    "    bestGuess = hundredthRange[lowestErrIndex]\n",
    "    return bestGuess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equation 11 from Curtis et al. (2002); Defines N as a function of Nmax, Nmin, and alpha.  \n",
    "def alphaFunction(a, N, Nmax, Nmin=1):\n",
    "    y = sqrt(pi*Nmin*Nmax)/(2.0*a) * exp((a * log2(sqrt(Nmax/Nmin)))**2.0)\n",
    "    y = y * exp((log(2.0)/(2.0*a))**2.0)\n",
    "    y = y * erf(a * log2(sqrt(Nmax/Nmin)) - log(2.0)/(2.0*a))\n",
    "    y += erf(a * log2(sqrt(Nmax/Nmin)) + log(2.0)/(2.0*a))\n",
    "    y -= N # subtract N to solve for a in getS function\n",
    "    return y\n",
    "\n",
    "# Equation 10 from Curtis et al. (2002); yields the total number of species\n",
    "def s2(a, Nmax, Nmin=1): \n",
    "    return sqrt(pi)/a * exp( (a * log2(sqrt(Nmax/Nmin)))**2)\n",
    "\n",
    "# Predict Nmax from scaling function Nmax = cNmax**z\n",
    "def getNmax(N, b, slope):\n",
    "    return 10 ** (b + slope*(log10(N)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the number of species by estimating Nmax, solving for alpha, and\n",
    "#def getS(Nrange, db, dz, guess, predictNmax=True):\n",
    "def getS(Nrange, db, dz, predictNmax=True,makeGuess=True):\n",
    "    Dlist = []\n",
    "    Slist_ln = []\n",
    "    Nlist = []\n",
    "\n",
    "    for i in range(1000):\n",
    "        N = float(np.random.uniform(Nrange)[1])\n",
    "        Nlist.append(N)\n",
    "\n",
    "        Nmax = 0\n",
    "        if predictNmax == True:\n",
    "            Nmax = getNmax(N, db, dz)\n",
    "        else:\n",
    "            Nmax = np.random.uniform(NmaxRange)[1] ### This is legacy from LL2016 code\n",
    "\n",
    "        Dlist.append(Nmax)\n",
    "        Nmin = 1\n",
    "        if makeGuess == False:\n",
    "            guess = 0.1060 # guess for global estimate from original LL2016 code \n",
    "        else:\n",
    "            guess = findGuess(N, Nmax, Nmin)\n",
    "        alpha = opt.fsolve(alphaFunction, guess, (N, Nmax, Nmin))[0]\n",
    " \n",
    "        S2 = s2(alpha, Nmax, 1)\n",
    "        Slist_ln.append(S2)\n",
    "\n",
    "    #print 'guess = %f' % guess\n",
    "    Slist_ln = log10(Slist_ln)\n",
    "\n",
    "    S_ln = np.mean(Slist_ln)\n",
    "    S_sorted = np.sort(Slist_ln)\n",
    "    \n",
    "    lowPred = S_sorted[24]\n",
    "    highPred = S_sorted[974]\n",
    "    return [S_ln, lowPred, highPred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitLL2016():\n",
    "    #############################################\n",
    "    ### Prepare LL2016 Data with subsurface data\n",
    "    #############################################\n",
    "    tail = str()\n",
    "    if ones is False:\n",
    "        tail = '-SADMetricData_NoMicrobe1s.txt'\n",
    "    elif ones is True:\n",
    "        tail = '-SADMetricData.txt'\n",
    "\n",
    "    datasets = []\n",
    "    GoodNames = []\n",
    "    emp = str()\n",
    "\n",
    "    if condition == 'open': emp = 'EMPopen'\n",
    "    elif condition == 'closed': emp = 'EMPclosed'\n",
    "\n",
    "    GoodNames = [emp, 'TARA', 'HMP', 'BIGN', 'BOVINE', 'CHU', 'LAUB', 'SED', 'HUMAN', 'CHINA', 'CATLIN', 'FUNGI']\n",
    "    GoodNames.append('subClosed')\n",
    "\n",
    "    its = 1\n",
    "    for name in os.listdir(mydir +'data/micro'):\n",
    "        if name in GoodNames: pass\n",
    "        else: continue\n",
    "\n",
    "        path = mydir+'data/micro/'+name+'/'+name+tail\n",
    "        numlines = sum(1 for line in open(path))\n",
    "        datasets.append([name, 'micro', numlines])\n",
    "\n",
    "    for name in os.listdir(mydir +'data/subsurface'):\n",
    "        if name in GoodNames: pass\n",
    "        else: continue\n",
    "\n",
    "        path = mydir+'data/subsurface/'+name+'/'+name+tail\n",
    "        num_lines = sum(1 for line in open(path))\n",
    "        datasets.append([name, 'subsurface', num_lines])\n",
    "\n",
    "\n",
    "    if sampling <= 500: its = 1000\n",
    "    else: its = 1000\n",
    "\n",
    "    for i in range(its):\n",
    "\n",
    "        Nlist, Slist, klist, NmaxList = [[],[],[],[]]\n",
    "\n",
    "        for dataset in datasets:\n",
    "            radDATA = []\n",
    "            name, kind, numlines = dataset\n",
    "            lines = []\n",
    "\n",
    "            small_mgrast = ['BIGN', 'BOVINE', 'CHU', 'LAUB', 'SED']\n",
    "            big_mgrast = ['HUMAN', 'CHINA', 'CATLIN', 'FUNGI', 'HYDRO']\n",
    "\n",
    "            if kind == 'micro':\n",
    "                if name in small_mgrast:\n",
    "                    lines = np.random.choice(range(1, numlines+1), 160, replace=True) # 40\n",
    "\n",
    "                elif name in big_mgrast:\n",
    "                    lines = np.random.choice(range(1, numlines+1), 400, replace=True) # 100\n",
    "\n",
    "                else:\n",
    "                    lines = np.random.choice(range(1, numlines+1), 400, replace=True) # 100\n",
    "                path = mydir+'data/micro/'+name+'/'+name+tail\n",
    "\n",
    "            elif kind == 'subsurface':\n",
    "                lines = range(1, numlines+1)\n",
    "                path = mydir+'data/subsurface/'+name+'/'+name+tail\n",
    "\n",
    "            for line in lines:\n",
    "                data = linecache.getline(path, line)\n",
    "                radDATA.append(data)\n",
    "\n",
    "            ct = 0\n",
    "            for data in radDATA:\n",
    "                data = data.split()\n",
    "                if data == []: continue\n",
    "                name, kind, N, S, Var, Evar, ESimp, EQ, O, ENee, EPielou, EHeip, BP, SimpDom, Nmax, McN, skew, logskew, chao1, ace, jknife1, jknife2, margalef, menhinick, preston_a, preston_S = data\n",
    "\n",
    "                N = float(N)\n",
    "                S = float(S)\n",
    "                Nmax = float(Nmax)\n",
    "\n",
    "                ct += 1\n",
    "                Nlist.append(float(np.log10(N)))\n",
    "                Slist.append(float(np.log10(S)))\n",
    "\n",
    "                NmaxList.append(float(np.log10(Nmax)))\n",
    "                klist.append('DarkCyan')\n",
    "\n",
    "        Nlist, Slist, NmaxList = zip(*sorted(zip(Nlist, Slist, NmaxList)))\n",
    "        Nlist = list(Nlist)\n",
    "        Slist = list(Slist)\n",
    "        NmaxList = list(NmaxList)\n",
    "\n",
    "        ####Regression for Dominance (Nmax) vs. N\n",
    "        d = pd.DataFrame({'N': Nlist})\n",
    "        d['Nmax'] = NmaxList\n",
    "        f = smf.ols('Nmax ~ N', d).fit()\n",
    "        return f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make species estimates\n",
    "def predictionFigure(condition, ones, sampling, f,scatterColor,scatterLabel): # f is the fit (all micro, Nmax_reads, Nmax_cells)\n",
    "\n",
    "\n",
    "    fs = 12 # font size used across figures\n",
    "    metric = 'Richness, '+r'$log$'+r'$_{10}$'\n",
    "    \n",
    "    d_blist = []\n",
    "    d_zlist = []\n",
    "\n",
    "\n",
    "\n",
    "    if f == 'LL2016':\n",
    "        for loceyIteration in range(1):\n",
    "            f = fitLL2016()\n",
    "            intercept = f.params[0]\n",
    "            slope = f.params[1]\n",
    "            d_blist.append(intercept)\n",
    "            d_zlist.append(slope)\n",
    "    else:\n",
    "        R2 = f.rsquared\n",
    "        intercept = f.params[0]\n",
    "        slope = f.params[1]\n",
    "\n",
    "        d_blist.append(intercept)\n",
    "        d_zlist.append(slope)\n",
    "\n",
    "    db = np.mean(d_blist)\n",
    "    dz = np.mean(d_zlist)\n",
    "    print 'Intercept: %f; Slope: %f' % (db,dz)\n",
    "\n",
    "    ################################################\n",
    "    ### All subsurface\n",
    "    myNrange = [2*(10**29), 6*(10**29)] # estimated subsurface by CM \n",
    "    #myNrange = [9.2*(10**29), 31.7*(10**29)] # global estimate used by LL2016; they used a guess = 0.1060\n",
    "   \n",
    "    #estimatedS, minS, maxS = getS(myNrange, db, dz, guess, predictNmax=True)\n",
    "    estimatedS, minS, maxS = getS(myNrange, db, dz, predictNmax=True,makeGuess=True)\n",
    "    print 'lognormal prediction of S for Continental Subsurface, using predicted Nmax:', '%.3e' % 10**estimatedS#S_ln\n",
    "    print '%.3e' % 10**minS#S_sorted[24]\n",
    "    print '%.3e' % 10**maxS#S_sorted[974]\n",
    "    #####################################\n",
    "    ###########\n",
    "    \n",
    "       \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ncells Nmax Scaling\n",
      "Intercept: 1.643944; Slope: 0.294239\n",
      "lognormal prediction of S for Continental Subsurface, using predicted Nmax: 9.637e+09\n",
      "2.485e+09\n",
      "1.592e+10\n",
      "Nreads Nmax Scaling\n",
      "Intercept: -0.583545; Slope: 0.997789\n",
      "lognormal prediction of S for Continental Subsurface, using predicted Nmax: 1.404e+09\n",
      "7.042e+08\n",
      "1.793e+09\n",
      "LL2016 Scaling\n",
      "Intercept: -0.408477; Slope: 0.927780\n",
      "lognormal prediction of S for Continental Subsurface, using predicted Nmax: 1.363e+12\n",
      "4.860e+11\n",
      "1.977e+12\n"
     ]
    }
   ],
   "source": [
    "mydir = os.path.expanduser(\"~/Desktop/ScalingMicroBiodiversity-master/\")\n",
    "mydir2 = os.path.expanduser(\"~/\")\n",
    "pi = math.pi\n",
    "cellDominance =  load_pickle(\"Ncells_Dominance_fit.pickle\")\n",
    "readDominance = load_pickle(\"Nreads_Dominance_fit.pickle\")\n",
    "\n",
    "df = pd.read_csv('Ncells_S_per_site.csv')\n",
    "otu = pd.read_csv('closed_reference_OTU_table.csv',header=None)\n",
    "\n",
    "\n",
    "EMPcondition = ['closed']\n",
    "Singletons = [True]\n",
    "Samplings = [1000]\n",
    "\n",
    "\n",
    "\n",
    "for condition in EMPcondition:\n",
    "    for ones in Singletons:\n",
    "        for sampling in Samplings:\n",
    "            print 'Ncells Nmax Scaling'        \n",
    "            predictionFigure(condition, ones, sampling,cellDominance,'maroon',r'$N_{max}$'+' scaling with '+r'$N_{cells}$')\n",
    "            print 'Nreads Nmax Scaling'\n",
    "            predictionFigure(condition, ones, sampling,readDominance, 'darkorange',r'$N_{max}$'+' scaling with '+r'$N_{reads}$')\n",
    "            print 'LL2016 Scaling'\n",
    "            predictionFigure(condition, ones, sampling,'LL2016','blue',r'$N_{max}$'+' from LL2016 and '+ r'$N_{reads}$')\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
